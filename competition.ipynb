{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Student Information\n",
    "Name: Edwin Sanjaya\n",
    "\n",
    "Student ID: 110065710\n",
    "\n",
    "GitHub ID: edwinsanjaya\n",
    "\n",
    "Kaggle name: Edwin Sanjaya Èô≥ÊΩ§ÁÉà\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png) (**Rank: 43/101**, Score: 0.45991)\n",
    "\n",
    "\n",
    "## Assignment 2 & 3\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm2022-isa5810-lab2-homework) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/2b0d14a829f340bc88d2660dc602d4bd. The scoring will be given according to your place in the Private Leaderboard ranking:\n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)\n",
    "    Submit your last submission __BEFORE the deadline (Nov. 22th 11:59 pm, Tuesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Read and Explore the Data\n",
    "\n",
    "The first step to work with the assignment is by reading the dataset that will be used in our lab. Pandas data frame will be used to store the data since it provide robust data structure and container to work on data analysis and data science\n",
    "\n",
    "These are the file that we are working on:\n",
    "- **data_identification.csv** provides information whether a certain tweet_id is a training or test dataset\n",
    "- **emotion.csv** provides emotion label of certain tweet_id. only training dataset has the labelled emotion, test dataset has null value and will be used to assess the model performance in Kaggle competition\n",
    "- **tweets_DM.json** provides the dataset of tweets in twitter in JSON format\n",
    "\n",
    "Sample of tweets_DM.json data:\n",
    "```\n",
    "{\n",
    "  \"_score\": 391,\n",
    "  \"_index\": \"hashtag_tweets\",\n",
    "  \"_source\": {\n",
    "    \"tweet\": {\n",
    "      \"hashtags\": [\n",
    "        \"Snapchat\"\n",
    "      ],\n",
    "      \"tweet_id\": \"0x376b20\",\n",
    "      \"text\": \"People who post \\\"add me on #Snapchat\\\" must be dehydrated. Cuz man.... that's <LH>\"\n",
    "    }\n",
    "  },\n",
    "  \"_crawldate\": \"2015-05-23 11:42:47\",\n",
    "  \"_type\": \"tweets\"\n",
    "}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 1.1 Reading the CSV files\n",
    "\n",
    "**data_identification.csv** and **emotion.csv** file can be directly loaded without further processing since CSV files already in table format"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "         tweet_id identification\n0        0x28cc61           test\n1        0x29e452          train\n2        0x2b3819          train\n3        0x2db41f           test\n4        0x2a2acc          train\n...           ...            ...\n1867530  0x227e25          train\n1867531  0x293813          train\n1867532  0x1e1a7e          train\n1867533  0x2156a5          train\n1867534  0x2bb9d2          train\n\n[1867535 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>identification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x28cc61</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x29e452</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x2b3819</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x2db41f</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2a2acc</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1867530</th>\n      <td>0x227e25</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1867531</th>\n      <td>0x293813</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1867532</th>\n      <td>0x1e1a7e</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1867533</th>\n      <td>0x2156a5</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1867534</th>\n      <td>0x2bb9d2</td>\n      <td>train</td>\n    </tr>\n  </tbody>\n</table>\n<p>1867535 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "kaggle_folder = os.path.join(os.getcwd(), 'kaggle_data')\n",
    "\n",
    "# Function to generate DataFrame via CSV filename\n",
    "def df_from_csv(filename):\n",
    "    f = os.path.join(kaggle_folder, filename)\n",
    "    return pd.read_csv(f, delimiter='\\t|\\n|,', engine='python')\n",
    "\n",
    "# Generate DataFrame\n",
    "data_identification = df_from_csv('data_identification.csv')\n",
    "emotion = df_from_csv('emotion.csv')\n",
    "data_identification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "         tweet_id       emotion\n0        0x3140b1       sadness\n1        0x368b73       disgust\n2        0x296183  anticipation\n3        0x2bd6e1           joy\n4        0x2ee1dd  anticipation\n...           ...           ...\n1455558  0x38dba0           joy\n1455559  0x300ea2           joy\n1455560  0x360b99          fear\n1455561  0x22eecf           joy\n1455562  0x2fb282  anticipation\n\n[1455563 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x3140b1</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x368b73</td>\n      <td>disgust</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x296183</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x2bd6e1</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2ee1dd</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455558</th>\n      <td>0x38dba0</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>1455559</th>\n      <td>0x300ea2</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>1455560</th>\n      <td>0x360b99</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>1455561</th>\n      <td>0x22eecf</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>1455562</th>\n      <td>0x2fb282</td>\n      <td>anticipation</td>\n    </tr>\n  </tbody>\n</table>\n<p>1455563 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 1.2 Reading JSON file\n",
    "\n",
    "Unlike CSV files, the JSON files contains the tweets data in nested JSON format. There are two major problem in the format:\n",
    "1. The JSON objects in the file separated by the newline, which is not supported by pandas reader\n",
    "2. The ```_source``` key contains nested object. Using ```pd.read_json``` or ```pd.DataFrame``` directly will cause the whole nested objects stored in one attribute.\n",
    "\n",
    "To solve the problem, two approaches was used:\n",
    "1. We iterate the JSON file line by line\n",
    "2. Pandas ```pd.json_normalize``` were used to convert the nested JSON into flat JSON (one level)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       _score          _index  \\\ntweet     391  hashtag_tweets   \n\n                                                 _source           _crawldate  \\\ntweet  {'hashtags': ['Snapchat'], 'tweet_id': '0x376b...  2015-05-23 11:42:47   \n\n        _type  \ntweet  tweets  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_score</th>\n      <th>_index</th>\n      <th>_source</th>\n      <th>_crawldate</th>\n      <th>_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tweet</th>\n      <td>391</td>\n      <td>hashtag_tweets</td>\n      <td>{'hashtags': ['Snapchat'], 'tweet_id': '0x376b...</td>\n      <td>2015-05-23 11:42:47</td>\n      <td>tweets</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Issue in read JSON data directly: nested object in _source aggregated as one in _source field\n",
    "test_json = {\n",
    "    \"_score\": 391,\n",
    "    \"_index\": \"hashtag_tweets\",\n",
    "    \"_source\": {\n",
    "        \"tweet\": {\n",
    "            \"hashtags\": [\n",
    "                \"Snapchat\"\n",
    "            ],\n",
    "            \"tweet_id\": \"0x376b20\",\n",
    "            \"text\": \"People who post \\\"add me on #Snapchat\\\" must be dehydrated. Cuz man.... that's <LH>\"\n",
    "        }\n",
    "    },\n",
    "    \"_crawldate\": \"2015-05-23 11:42:47\",\n",
    "    \"_type\": \"tweets\"\n",
    "}\n",
    "test_json_df = pd.DataFrame(test_json)\n",
    "test_json_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method NDFrame.head of          tweet_id                                               text\n0        0x376b20  People who post \"add me on #Snapchat\" must be ...\n1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...\n2        0x28b412  Confident of your obedience, I write to you, k...\n3        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>\n4        0x2de201  \"Trust is not the same as faith. A friend is s...\n...           ...                                                ...\n1867530  0x316b80  When you buy the last 2 tickets remaining for ...\n1867531  0x29d0cb  I swear all this hard work gone pay off one da...\n1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...\n1867533  0x24faed  Ah, corporate life, where you can date <LH> us...\n1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>\n\n[1867535 rows x 2 columns]>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON file, iterate line by line, store the data in list\n",
    "f = open(os.path.join(kaggle_folder, 'tweets_DM.json'))\n",
    "data = []\n",
    "for line in f:\n",
    "    data.append(json.loads(line))\n",
    "\n",
    "# Normalize nested JSON and convert to DataFrame\n",
    "tweets_dm = pd.json_normalize(data)\n",
    "\n",
    "# Rename columns in more readable format\n",
    "rename_map = {\n",
    "    '_score': 'score',\n",
    "    '_index': 'index',\n",
    "    '_crawldate' : 'date',\n",
    "    '_type': 'type',\n",
    "    '_source.tweet.hashtags': 'hashtags',\n",
    "    '_source.tweet.tweet_id': 'tweet_id',\n",
    "    '_source.tweet.text': 'text'\n",
    "}\n",
    "tweets_dm.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Get necessary data: the tweet_id and text\n",
    "tweets_dm = tweets_dm[['tweet_id', 'text']]\n",
    "tweets_dm.head"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 1.3 Data Exploration\n",
    "\n",
    "Data needs to be explored to find some basic insights on how to work with the data. The exploration includes:\n",
    "- Checking the dimension\n",
    "- Checking the emotion lists\n",
    "- Checking the null value\n",
    "- Checking the duplicate value\n",
    "- Sampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data identification: (1867535, 2)\n",
      "Emotion: (1455563, 2)\n",
      "Tweets DM: (1867535, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking dimension (good practice :3)\n",
    "print(f'Data identification: {data_identification.shape}')\n",
    "print(f'Emotion: {emotion.shape}')\n",
    "print(f'Tweets DM: {tweets_dm.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "              tweet_id\nemotion               \nanger            39867\nanticipation    248935\ndisgust         139101\nfear             63999\njoy             516017\nsadness         193437\nsurprise         48729\ntrust           205478",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n    </tr>\n    <tr>\n      <th>emotion</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>anger</th>\n      <td>39867</td>\n    </tr>\n    <tr>\n      <th>anticipation</th>\n      <td>248935</td>\n    </tr>\n    <tr>\n      <th>disgust</th>\n      <td>139101</td>\n    </tr>\n    <tr>\n      <th>fear</th>\n      <td>63999</td>\n    </tr>\n    <tr>\n      <th>joy</th>\n      <td>516017</td>\n    </tr>\n    <tr>\n      <th>sadness</th>\n      <td>193437</td>\n    </tr>\n    <tr>\n      <th>surprise</th>\n      <td>48729</td>\n    </tr>\n    <tr>\n      <th>trust</th>\n      <td>205478</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking unique emotion value\n",
    "emotion.groupby('emotion').nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Preprocessing\n",
    "\n",
    "One of the process to improve our emotion prediction is by doing pre-processing.\n",
    "\n",
    "The following preprocessing techniques were used in this assignment:\n",
    "- Lowercase\n",
    "- Regular Expression: remove symbol, number, punctuation\n",
    "- Tokenization\n",
    "- Stopword Removal\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "\n",
    "Following library was used:\n",
    "- Regex from Python\n",
    "- Tokenizer, Stopword, Stemmer, Lemmatizer from NLTK\n",
    "- Lemmatizer from Spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Edwin\n",
      "[nltk_data]     Sanjaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Edwin\n",
      "[nltk_data]     Sanjaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Edwin Sanjaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Required library for pre-processing\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import preprocessor as p\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Require: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.1 Basic Preprocessing: Lowercase, Banned Word, Regex\n",
    "\n",
    "The first pre-processing part included as follow:\n",
    "- Remove case sensitivity with lowercase\n",
    "- Remove banned word such as RT & <LH> because it's common in twitter, however didn't provide much information about the emotion\n",
    "- Remove symbols, numbers and repeating characters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Remove mention, url, symbol\n",
    "def basic_tweet_preprocess(text):\n",
    "    output = text.lower()\n",
    "    bw_list = ['<lh>', 'rt']\n",
    "    for bw in bw_list:\n",
    "        output = output.replace(bw, '')\n",
    "    output = re.sub(r'[!#$%^&*(),.?\":;{}|<>_]', '', output)\n",
    "    output = re.sub(r'(.)1+', r'1', output)\n",
    "    output = re.sub(r'[0-9]+', '', output)\n",
    "    output = re.sub(r\"'\", '', output)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2 Tokenization\n",
    "\n",
    "Normally, we can use common build-in tokenizer in nltk which is word_tokenize. After some exploration and research nltk has build-in tokenizer which specialized on working on Twitter's text data called **TweetTokenizer()**. The specified tokenizer will be used for this lab assignment since they have the following benefits:\n",
    "- Tokenize the word just like normal tokenizer\n",
    "- Detect handles a.k.a. username info that started with @\n",
    "- Detect repeated characters\n",
    "- Detect phone numbers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def tweet_tokenize(text):\n",
    "    tt = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    return tt.tokenize(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.3 Stopword Removal\n",
    "\n",
    "Stopword removal is an important process to reduce the number of redundant word that does not provide any clue in the emotion analysis. This process help our to focus on the important word which provide more information about the emotion of the tweet. Additionally, by reducing the number of words, it requires less processing time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    sw = set(stopwords.words(\"english\"))\n",
    "    output = [w for w in text if not w in sw]\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.4 Stemming"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    output = text\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    output = [stemmer.stem(w) for w in text]\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.5 Lemmatizing\n",
    "\n",
    "nltk and spacy provides lemmatization library. However a part of speech tagging (pos_tag) is necessary to optimize the quality of the lemmatization process. Compared to the stemming, lemmatization also requires more processing time with relatively similar result.\n",
    "\n",
    "After some research in several external sources and experiment. It is found that Spacy already have built-in part of speech and more robust compared with NLTK version. Therefore this experiment uses Spacy for the Lemmatizing process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ndef lemmatizing(text):\\n    output = []\\n    lemmatizer = nltk.WordNetLemmatizer()\\n    text_pos = pos_tag(text)\\n    for token_pos in text_pos:\\n        output.append(lemmatizer.lemmatize(token_pos[0], pos=tag_map[token_pos[1]]))\\n    return output\\n'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spacy's Lemmatizer\n",
    "def lemmatizing(text):\n",
    "    text = nlp(\" \".join(text))\n",
    "    output = []\n",
    "    for token in text:\n",
    "        output.append(token.lemma_)\n",
    "    return output\n",
    "\n",
    "# NLTK's Lemmatizer\n",
    "\"\"\"\n",
    "def lemmatizing(text):\n",
    "    output = []\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    text_pos = pos_tag(text)\n",
    "    for token_pos in text_pos:\n",
    "        output.append(lemmatizer.lemmatize(token_pos[0], pos=tag_map[token_pos[1]]))\n",
    "    return output\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def join_token(list):\n",
    "    output = ' '.join(list)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Example: Pre-processing step by step in first 5 data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "   tweet_id                                               text\n0  0x376b20  People who post \"add me on #Snapchat\" must be ...\n1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...\n2  0x28b412  Confident of your obedience, I write to you, k...\n3  0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>\n4  0x2de201  \"Trust is not the same as faith. A friend is s...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x376b20</td>\n      <td>People who post \"add me on #Snapchat\" must be ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2d5350</td>\n      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x28b412</td>\n      <td>Confident of your obedience, I write to you, k...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x1cd5b0</td>\n      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2de201</td>\n      <td>\"Trust is not the same as faith. A friend is s...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data to show step by step process\n",
    "head_tweets_dm = tweets_dm.head().copy()\n",
    "head_tweets_dm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "   tweet_id                                               text\n0  0x376b20  people who post add me on snapchat must be deh...\n1  0x2d5350  @brianklaas as we see trump is dangerous to fr...\n2  0x28b412  confident of your obedience i write to you kno...\n3  0x1cd5b0                    now issa is stalking tasha üòÇüòÇüòÇ \n4  0x2de201  trust is not the same as faith a friend is som...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x376b20</td>\n      <td>people who post add me on snapchat must be deh...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2d5350</td>\n      <td>@brianklaas as we see trump is dangerous to fr...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x28b412</td>\n      <td>confident of your obedience i write to you kno...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x1cd5b0</td>\n      <td>now issa is stalking tasha üòÇüòÇüòÇ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2de201</td>\n      <td>trust is not the same as faith a friend is som...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tweets_dm['text'] = head_tweets_dm['text'].apply(lambda x: basic_tweet_preprocess(x))\n",
    "head_tweets_dm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "   tweet_id                                               text\n0  0x376b20  [people, who, post, add, me, on, snapchat, mus...\n1  0x2d5350  [as, we, see, trump, is, dangerous, to, freepr...\n2  0x28b412  [confident, of, your, obedience, i, write, to,...\n3  0x1cd5b0          [now, issa, is, stalking, tasha, üòÇ, üòÇ, üòÇ]\n4  0x2de201  [trust, is, not, the, same, as, faith, a, frie...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x376b20</td>\n      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2d5350</td>\n      <td>[as, we, see, trump, is, dangerous, to, freepr...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x28b412</td>\n      <td>[confident, of, your, obedience, i, write, to,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x1cd5b0</td>\n      <td>[now, issa, is, stalking, tasha, üòÇ, üòÇ, üòÇ]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2de201</td>\n      <td>[trust, is, not, the, same, as, faith, a, frie...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "head_tweets_dm['text'] = head_tweets_dm['text'].apply(lambda x: tweet_tokenize(x))\n",
    "head_tweets_dm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "   tweet_id                                               text\n0  0x376b20  [people, post, add, snapchat, must, dehydrated...\n1  0x2d5350  [see, trump, dangerous, freepress, around, wor...\n2  0x28b412  [confident, obedience, write, knowing, even, a...\n3  0x1cd5b0                   [issa, stalking, tasha, üòÇ, üòÇ, üòÇ]\n4  0x2de201  [trust, faith, friend, someone, trust, putting...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x376b20</td>\n      <td>[people, post, add, snapchat, must, dehydrated...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2d5350</td>\n      <td>[see, trump, dangerous, freepress, around, wor...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x28b412</td>\n      <td>[confident, obedience, write, knowing, even, a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x1cd5b0</td>\n      <td>[issa, stalking, tasha, üòÇ, üòÇ, üòÇ]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2de201</td>\n      <td>[trust, faith, friend, someone, trust, putting...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop word\n",
    "head_tweets_dm['text'] = head_tweets_dm['text'].apply(lambda x: remove_stopwords(x))\n",
    "head_tweets_dm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "   tweet_id                                               text\n0  0x376b20  [peopl, post, add, snapchat, must, dehydr, cuz...\n1  0x2d5350  [see, trump, danger, freepress, around, world,...\n2  0x28b412  [confid, obedi, write, know, even, ask, philem...\n3  0x1cd5b0                      [issa, stalk, tasha, üòÇ, üòÇ, üòÇ]\n4  0x2de201  [trust, faith, friend, someon, trust, put, fai...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x376b20</td>\n      <td>[peopl, post, add, snapchat, must, dehydr, cuz...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2d5350</td>\n      <td>[see, trump, danger, freepress, around, world,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x28b412</td>\n      <td>[confid, obedi, write, know, even, ask, philem...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x1cd5b0</td>\n      <td>[issa, stalk, tasha, üòÇ, üòÇ, üòÇ]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2de201</td>\n      <td>[trust, faith, friend, someon, trust, put, fai...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem\n",
    "head_tweets_dm['text'] = head_tweets_dm['text'].apply(lambda x: stemming(x))\n",
    "head_tweets_dm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "   tweet_id                                               text\n0  0x376b20  [peopl, post, add, snapchat, must, dehydr, cuz...\n1  0x2d5350  [see, trump, danger, freepress, around, world,...\n2  0x28b412  [confid, obedi, write, know, even, ask, philem...\n3  0x1cd5b0                      [issa, stalk, tasha, üòÇ, üòÇ, üòÇ]\n4  0x2de201  [trust, faith, friend, someon, trust, put, fai...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x376b20</td>\n      <td>[peopl, post, add, snapchat, must, dehydr, cuz...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2d5350</td>\n      <td>[see, trump, danger, freepress, around, world,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x28b412</td>\n      <td>[confid, obedi, write, know, even, ask, philem...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x1cd5b0</td>\n      <td>[issa, stalk, tasha, üòÇ, üòÇ, üòÇ]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2de201</td>\n      <td>[trust, faith, friend, someon, trust, put, fai...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemma\n",
    "head_tweets_dm['text'] = head_tweets_dm['text'].apply(lambda x: lemmatizing(x))\n",
    "head_tweets_dm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Finally, we applied the whole pre-processing process to our real datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "         tweet_id                                               text\n0        0x376b20  [peopl, post, add, snapchat, must, dehydr, cuz...\n1        0x2d5350  [see, trump, danger, freepress, around, world,...\n2        0x28b412  [confid, obedi, write, know, even, ask, philem...\n3        0x1cd5b0                      [issa, stalk, tasha, üòÇ, üòÇ, üòÇ]\n4        0x2de201  [trust, faith, friend, someon, trust, put, fai...\n...           ...                                                ...\n1867530  0x316b80  [buy, last, ticket, remain, show, sell, mixedf...\n1867531  0x29d0cb    [swear, hard, work, go, pay, one, day, üòà, üí∞, üí∏]\n1867532  0x2a6a4f          [card, leave, be, not, idea, get, parcel]\n1867533  0x24faed  [ah, corpor, life, date, use, rel, anachron, l...\n1867534  0x34be8c                           [bless, live, sundayvib]\n\n[1867535 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x376b20</td>\n      <td>[peopl, post, add, snapchat, must, dehydr, cuz...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2d5350</td>\n      <td>[see, trump, danger, freepress, around, world,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x28b412</td>\n      <td>[confid, obedi, write, know, even, ask, philem...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x1cd5b0</td>\n      <td>[issa, stalk, tasha, üòÇ, üòÇ, üòÇ]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2de201</td>\n      <td>[trust, faith, friend, someon, trust, put, fai...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1867530</th>\n      <td>0x316b80</td>\n      <td>[buy, last, ticket, remain, show, sell, mixedf...</td>\n    </tr>\n    <tr>\n      <th>1867531</th>\n      <td>0x29d0cb</td>\n      <td>[swear, hard, work, go, pay, one, day, üòà, üí∞, üí∏]</td>\n    </tr>\n    <tr>\n      <th>1867532</th>\n      <td>0x2a6a4f</td>\n      <td>[card, leave, be, not, idea, get, parcel]</td>\n    </tr>\n    <tr>\n      <th>1867533</th>\n      <td>0x24faed</td>\n      <td>[ah, corpor, life, date, use, rel, anachron, l...</td>\n    </tr>\n    <tr>\n      <th>1867534</th>\n      <td>0x34be8c</td>\n      <td>[bless, live, sundayvib]</td>\n    </tr>\n  </tbody>\n</table>\n<p>1867535 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequential pre-processing\n",
    "tweets_dm['text'] = tweets_dm['text'].apply(lambda x: x.lower())\n",
    "tweets_dm['text'] = tweets_dm['text'].apply(lambda x: basic_tweet_preprocess(x))\n",
    "tweets_dm['text'] = tweets_dm['text'].apply(lambda x: tweet_tokenize(x))\n",
    "tweets_dm['text'] = tweets_dm['text'].apply(lambda x: remove_stopwords(x))\n",
    "tweets_dm['text'] = tweets_dm['text'].apply(lambda x: stemming(x))\n",
    "tweets_dm['text'] = tweets_dm['text'].apply(lambda x: lemmatizing(x))\n",
    "tweets_dm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "         tweet_id                                               text\n0        0x376b20   peopl post add snapchat must dehydr cuz man that\n1        0x2d5350  see trump danger freepress around world trumpl...\n2        0x28b412  confid obedi write know even ask philemon / bi...\n3        0x1cd5b0                             issa stalk tasha üòÇ üòÇ üòÇ\n4        0x2de201  trust faith friend someon trust put faith anyo...\n...           ...                                                ...\n1867530  0x316b80  buy last ticket remain show sell mixedfeel but...\n1867531  0x29d0cb               swear hard work go pay one day üòà üí∞ üí∏\n1867532  0x2a6a4f                  card leave be not idea get parcel\n1867533  0x24faed  ah corpor life date use rel anachron last job ...\n1867534  0x34be8c                               bless live sundayvib\n\n[1867535 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x376b20</td>\n      <td>peopl post add snapchat must dehydr cuz man that</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2d5350</td>\n      <td>see trump danger freepress around world trumpl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x28b412</td>\n      <td>confid obedi write know even ask philemon / bi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x1cd5b0</td>\n      <td>issa stalk tasha üòÇ üòÇ üòÇ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2de201</td>\n      <td>trust faith friend someon trust put faith anyo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1867530</th>\n      <td>0x316b80</td>\n      <td>buy last ticket remain show sell mixedfeel but...</td>\n    </tr>\n    <tr>\n      <th>1867531</th>\n      <td>0x29d0cb</td>\n      <td>swear hard work go pay one day üòà üí∞ üí∏</td>\n    </tr>\n    <tr>\n      <th>1867532</th>\n      <td>0x2a6a4f</td>\n      <td>card leave be not idea get parcel</td>\n    </tr>\n    <tr>\n      <th>1867533</th>\n      <td>0x24faed</td>\n      <td>ah corpor life date use rel anachron last job ...</td>\n    </tr>\n    <tr>\n      <th>1867534</th>\n      <td>0x34be8c</td>\n      <td>bless live sundayvib</td>\n    </tr>\n  </tbody>\n</table>\n<p>1867535 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join token to whole sentence\n",
    "tweets_dm['text'] = tweets_dm['text'].apply(lambda x: join_token(x))\n",
    "tweets_dm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3.1 Preparing the Training Dataset\n",
    "\n",
    "To receive the training datasets:\n",
    "- Merge the **tweets_dm** dataframe with the **emotion** dataframe. This is used to label the tweet with corresponding emotion\n",
    "- Merge the previous dataframe with the **data_identification** dataframe, only if the identification value is \"train\". This is used to filter the training datasets from the whole datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "         tweet_id                                               text  \\\n0        0x376b20   peopl post add snapchat must dehydr cuz man that   \n1        0x2d5350  see trump danger freepress around world trumpl...   \n2        0x1cd5b0                             issa stalk tasha üòÇ üòÇ üòÇ   \n3        0x1d755c  thx good time tonight stori heabreakingli auth...   \n4        0x2c91a8                            still wait suppli liscu   \n...           ...                                                ...   \n1455558  0x321566             I m happi nowond name show happi üëè üëè üëè   \n1455559  0x38959e  everi circumt i d like thank almighti jesu christ   \n1455560  0x2cbca6  there current two girl walk around librari han...   \n1455561  0x24faed  ah corpor life date use rel anachron last job ...   \n1455562  0x34be8c                               bless live sundayvib   \n\n              emotion identification  \n0        anticipation          train  \n1             sadness          train  \n2                fear          train  \n3                 joy          train  \n4        anticipation          train  \n...               ...            ...  \n1455558           joy          train  \n1455559           joy          train  \n1455560           joy          train  \n1455561           joy          train  \n1455562           joy          train  \n\n[1455563 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>text</th>\n      <th>emotion</th>\n      <th>identification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x376b20</td>\n      <td>peopl post add snapchat must dehydr cuz man that</td>\n      <td>anticipation</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2d5350</td>\n      <td>see trump danger freepress around world trumpl...</td>\n      <td>sadness</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x1cd5b0</td>\n      <td>issa stalk tasha üòÇ üòÇ üòÇ</td>\n      <td>fear</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x1d755c</td>\n      <td>thx good time tonight stori heabreakingli auth...</td>\n      <td>joy</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2c91a8</td>\n      <td>still wait suppli liscu</td>\n      <td>anticipation</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455558</th>\n      <td>0x321566</td>\n      <td>I m happi nowond name show happi üëè üëè üëè</td>\n      <td>joy</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1455559</th>\n      <td>0x38959e</td>\n      <td>everi circumt i d like thank almighti jesu christ</td>\n      <td>joy</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1455560</th>\n      <td>0x2cbca6</td>\n      <td>there current two girl walk around librari han...</td>\n      <td>joy</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1455561</th>\n      <td>0x24faed</td>\n      <td>ah corpor life date use rel anachron last job ...</td>\n      <td>joy</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1455562</th>\n      <td>0x34be8c</td>\n      <td>bless live sundayvib</td>\n      <td>joy</td>\n      <td>train</td>\n    </tr>\n  </tbody>\n</table>\n<p>1455563 rows √ó 4 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the train dataframe\n",
    "train_df = pd.merge(tweets_dm, emotion, on='tweet_id', how='inner')\n",
    "train_df = pd.merge(train_df, data_identification, on='tweet_id', how='inner')\n",
    "train_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3.2 Feature Extraction with TF-IDF\n",
    "\n",
    "For feature extraction in text mining. there are two common methods which is Bag of Words (CountVectorizer) or TF-IDF. Based on previous lab, exploration on external resources which implied that TF-IDF provides better results due to provides a penalty to the common word that appears in a lot of document (e.g.: is, a, are). So that, it is decided that TF-IDF methods will be used for the feature extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_dtm = tfidf_vect.fit(train_df['text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['aa', 'aaa', 'aaab', ..., 'ùñíùñäùñâùñéùñàùñéùñìùñä', 'ùñîùñì', 'ùñôùñçùñä'], dtype=object)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check terms\n",
    "tfidf_vect.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Machine Learning Modelling\n",
    "\n",
    "For our learning we will test the model to work with our datasets:\n",
    "- Multinomial Naive Bayes: We uses this Naive Bayes methods that provides better results compared to the other Naive Bayes methods in previous experiment.\n",
    "- Linear Support Vector Classifier\n",
    "- Logistic regression\n",
    "- Stochastic Gradient Descent Classifier\n",
    "\n",
    "Most of the methods that were used is commonly used for classification task. However they are not specialized in text processing since they don't capture the semantic of the sentence and relation between the words properly. However, these techniques able to process the data faster compared to the deep learning and language model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "          index  tweet_id                                               text\n0             2  0x28b412  confid obedi write know even ask philemon / bi...\n1             4  0x2de201  trust faith friend someon trust put faith anyo...\n2             9  0x218443  enough satisfi goal realli money materi money ...\n3            30  0x2939d5               god wake chase day godsplan godswork\n4            33  0x26289a                        tough time turn symbol hope\n...         ...       ...                                                ...\n411967  1867525  0x2913b4       messag ye hear begin love one anoth john kjv\n411968  1867529  0x2a980e  lad hath five barley loav two small fish among...\n411969  1867530  0x316b80  buy last ticket remain show sell mixedfeel but...\n411970  1867531  0x29d0cb               swear hard work go pay one day üòà üí∞ üí∏\n411971  1867532  0x2a6a4f                  card leave be not idea get parcel\n\n[411972 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>0x28b412</td>\n      <td>confid obedi write know even ask philemon / bi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>0x2de201</td>\n      <td>trust faith friend someon trust put faith anyo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n      <td>0x218443</td>\n      <td>enough satisfi goal realli money materi money ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30</td>\n      <td>0x2939d5</td>\n      <td>god wake chase day godsplan godswork</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33</td>\n      <td>0x26289a</td>\n      <td>tough time turn symbol hope</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>411967</th>\n      <td>1867525</td>\n      <td>0x2913b4</td>\n      <td>messag ye hear begin love one anoth john kjv</td>\n    </tr>\n    <tr>\n      <th>411968</th>\n      <td>1867529</td>\n      <td>0x2a980e</td>\n      <td>lad hath five barley loav two small fish among...</td>\n    </tr>\n    <tr>\n      <th>411969</th>\n      <td>1867530</td>\n      <td>0x316b80</td>\n      <td>buy last ticket remain show sell mixedfeel but...</td>\n    </tr>\n    <tr>\n      <th>411970</th>\n      <td>1867531</td>\n      <td>0x29d0cb</td>\n      <td>swear hard work go pay one day üòà üí∞ üí∏</td>\n    </tr>\n    <tr>\n      <th>411971</th>\n      <td>1867532</td>\n      <td>0x2a6a4f</td>\n      <td>card leave be not idea get parcel</td>\n    </tr>\n  </tbody>\n</table>\n<p>411972 rows √ó 3 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the test dataframe\n",
    "test_df = pd.merge(tweets_dm, data_identification, on='tweet_id', how='inner')\n",
    "test_df = test_df[test_df['identification']=='test']\n",
    "test_df = test_df[['tweet_id', 'text']]\n",
    "test_df = test_df.reset_index()\n",
    "\n",
    "# Save to pickle\n",
    "train_df.to_pickle(\"kaggle_train_df.pkl\")\n",
    "test_df.to_pickle(\"kaggle_test_df.pkl\")\n",
    "test_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 11.5min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Training the model\n",
    "x_train = tfidf_dtm.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "x_test = tfidf_dtm.transform(test_df['text'])\n",
    "\n",
    "# MNB_model = MultinomialNB()\n",
    "# MNB_model = MNB_model.fit(x_train, y_train)\n",
    "# LSVC_model = LinearSVC(verbose=True)\n",
    "# LSVC_model = LSVC_model.fit(x_train, y_train)\n",
    "LR_model = LogisticRegression(max_iter=850, n_jobs=-1, verbose=True)\n",
    "LR_model = LR_model.fit(x_train, y_train)\n",
    "# SGDC_model = SGDClassifier(max_iter=4000)\n",
    "# SGDC_model = SGDC_model.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Analyze & Generate the Result\n",
    "\n",
    "Sometimes the result of the training accuracy doesn't fully reflect the test accuracy. For example:\n",
    "- Using Naive Bayes, we got 69% Training accuracy, however our test accuracy only 37%\n",
    "- Using Linear Support Vector Classifier, we got 77% Training accuracy, however our test accuracy is 45%\n",
    "- Using Logistic Regression, we got 68% accuracy, however the test accuracy is higher 46%\n",
    "- Using Stochastic Gradient Descent Classifier, we got worse accuracy in both training and test accuracy compared to the Logistic Regression\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.5872998970157939\n"
     ]
    }
   ],
   "source": [
    "# Get traaining accuracy of the model\n",
    "selected_model = LR_model\n",
    "y_train_predict = selected_model.predict(x_train)\n",
    "y_test_predict = selected_model.predict(x_test)\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, y_train_predict)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "             emotion\n0       anticipation\n1       anticipation\n2                joy\n3       anticipation\n4              trust\n...              ...\n411967  anticipation\n411968  anticipation\n411969       sadness\n411970           joy\n411971       sadness\n\n[411972 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>trust</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>411967</th>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>411968</th>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>411969</th>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>411970</th>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>411971</th>\n      <td>sadness</td>\n    </tr>\n  </tbody>\n</table>\n<p>411972 rows √ó 1 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate data frame for the prediction\n",
    "y_test_predict = pd.DataFrame(y_test_predict, columns = ['emotion'])\n",
    "y_test_predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "              id       emotion\n0       0x28b412  anticipation\n1       0x2de201  anticipation\n2       0x218443           joy\n3       0x2939d5  anticipation\n4       0x26289a         trust\n...          ...           ...\n411967  0x2913b4  anticipation\n411968  0x2a980e  anticipation\n411969  0x316b80       sadness\n411970  0x29d0cb           joy\n411971  0x2a6a4f       sadness\n\n[411972 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x28b412</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2de201</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x218443</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x2939d5</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x26289a</td>\n      <td>trust</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>411967</th>\n      <td>0x2913b4</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>411968</th>\n      <td>0x2a980e</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>411969</th>\n      <td>0x316b80</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>411970</th>\n      <td>0x29d0cb</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>411971</th>\n      <td>0x2a6a4f</td>\n      <td>sadness</td>\n    </tr>\n  </tbody>\n</table>\n<p>411972 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename column to meet Kaggle specification\n",
    "submit_df = test_df.assign(emotion=y_test_predict)\n",
    "submit_df = submit_df[['tweet_id', 'emotion']]\n",
    "submit_df = submit_df.rename(columns={'tweet_id': 'id'})\n",
    "submit_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Create CSV file of test dataset emotion prediction\n",
    "submit_df.to_csv('submit.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Conclusion and Improvement\n",
    "\n",
    "Based on the process and results in this assignment, there are several improvement points that can be considered in the future:\n",
    "- Since we are working on a dataset with varying language, a translation library or API can be considered to standardize the text\n",
    "- Other way to work with the data is to classify the language in each text and create different training model for each language\n",
    "- We can provide more text pre-processing by using Regular Expression, especially to work on non-alphabetic and non-standard word\n",
    "- Convert emoji into text that represent the emotion\n",
    "- Improve the Lemmatization process by considering more robust part of speech tag\n",
    "- Use the model that are able to capture the semantic of the sentence (not only based on the feature), such as deep learning or language model (e.g.: Transformers, BERT)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
